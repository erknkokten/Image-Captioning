{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We provided 3 .py and 3 .ipynb file. If you want to run the code, you can run it in Google Colab.\n",
        "To do so, the Google Drive link which includes the dataset paths alongside with model check points is given below.\n",
        "https://drive.google.com/drive/folders/1rsKAKgeTv3WeX5A_hko5w8j2H4RdNcfU?usp=sharing\n",
        "\n",
        "IMPORTANT NOTE: YOU CANNOT RUN THIS CODE IN YOUR COMPUTER, OR EVEN IN YOUR OWN COLAB ENVIRONMENT. YOU NEED TO HAVE .pkl AND .h5 FILES TO PROCEED. FOR THE FINAL PROJECT OF EE443 COURSE, EVERYTHING YOU NEED IS AVAILABLE IN THE DRIVE LINK PROVIDED ABOVE. CONNECT IT IN YOUR COLAB ENVIRONMENT, AND YOU ARE READY TO GO.\n",
        "\n",
        "This code is now set to be using the latest checkpoint, only for **TESTING PURPOSES**. In this way, the code will not train anything. If you want to see training results, turn the flag below **False**.\n",
        "\n",
        "\n",
        "1-Links to other Google Colab environments\n",
        "\n",
        "1.   GRU + Attention Model:\n",
        "https://colab.research.google.com/drive/1Q4ueHEDtLcCVnZBcPsjfo1VWOcGua2cN?usp=sharing\n",
        "2.   GRU + Attention Model with GloVE word embedding: \n",
        "https://colab.research.google.com/drive/1VmJmFsE5JW0Hno3oLbKnfH5fDqnsuMjW?usp=sharing\n",
        "3.   LSTM Model: \n",
        "https://colab.research.google.com/drive/1_haPqb1kS1oVQfQSURitltkC5faJTmnn?usp=sharing\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_b37DS4n3HeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########IF YOU WANT TO TRAIN, SET THE FOLLOWING FLAG FALSE##############\n",
        "flag = True\n"
      ],
      "metadata": {
        "id": "hep8nUOW3JpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXHSx8yBquVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2c6bb3-6638-43dc-c0aa-a3c0197e324d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J0p-WSPwoTE"
      },
      "outputs": [],
      "source": [
        "zip_path = '/content/drive/MyDrive/Dataset/test_data.zip'\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q test_data.zip\n",
        "!rm test_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0P3y1WjWsS8"
      },
      "outputs": [],
      "source": [
        "zip_path = '/content/drive/MyDrive/Dataset/train_data.zip'\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q train_data.zip\n",
        "!rm train_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j38b-UyVI79_"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "import collections\n",
        "import random\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications import InceptionV3, InceptionResNetV2, EfficientNetB7, efficientnet\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import tensorflow.keras.utils\n",
        "import h5py\n",
        "import numpy as np\n",
        "import requests\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbOZSwY7WbgH"
      },
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBMBBIO43NTT"
      },
      "outputs": [],
      "source": [
        "test_img_path = np.array(listdir(\"/content/content/drive/MyDrive/Dataset/test\"))\n",
        "#train_img_path =np.array(listdir(\"train\"))\n",
        "#for deleting the non-existent images\n",
        "a = None\n",
        "for i in test_img_path:\n",
        "    try:\n",
        "        a = imread(\"/content/content/drive/MyDrive/Dataset/test/\"+ i )\n",
        "    except:\n",
        "        os.remove(\"/content/content/drive/MyDrive/Dataset/test/\"+ i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1CSFAT9Yptw"
      },
      "outputs": [],
      "source": [
        "train_img_path = np.array(listdir(\"/content/content/drive/MyDrive/Dataset/train\"))\n",
        "#train_img_path =np.array(listdir(\"train\"))\n",
        "\n",
        "#for deleting the non-existent images\n",
        "a = None\n",
        "for i in train_img_path:\n",
        "    try:\n",
        "        a = imread(\"/content/content/drive/MyDrive/Dataset/train/\"+ i )\n",
        "    except:\n",
        "        os.remove(\"/content/content/drive/MyDrive/Dataset/train/\"+ i)\n",
        "os.remove(\"/content/content/drive/MyDrive/Dataset/train/59999 (1).jpg\")\n",
        "os.remove(\"/content/content/drive/MyDrive/Dataset/train/82782 (1).jpg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swv5XuysI8Qn"
      },
      "outputs": [],
      "source": [
        "def load_image_inception(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    img_index = int(image_path.numpy().decode().split('/')[-1].split('.')[0])\n",
        "    return img, img_index\n",
        "\n",
        "def load_image_inceptionEval(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    img_index = int(image_path.split('/')[-1].split('.')[0])\n",
        "    return img, img_index\n",
        "\n",
        "def load_image_efNet(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (600, 600))\n",
        "    img = tf.image.convert_image_dtype(img,dtype=tf.float32)\n",
        "    img = efficientnet.preprocess_input(img)\n",
        "    img_index = int(image_path.numpy().decode().split('\\\\')[1].split('.')[0])\n",
        "    return img, img_index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he3Yjo7TI8cS"
      },
      "outputs": [],
      "source": [
        "train = h5py.File('/content/drive/MyDrive/Dataset/eee443_project_dataset_train.h5', 'r')\n",
        "test = h5py.File('/content/drive/MyDrive/Dataset/eee443_project_dataset_test.h5', 'r')\n",
        "#Captions for training images\n",
        "train_cap = train['train_cap']\n",
        "train_cap = np.array(train_cap)\n",
        "#The indices of training images\n",
        "train_imid = train['train_imid']\n",
        "train_imid = np.array(train_imid) -1\n",
        "#Pretrained feature vector for training images !!!USING IT WILL HALVE THE GRADE!!!!\n",
        "train_ims = train['train_ims']\n",
        "train_ims = np.array(train_ims)\n",
        "#Flickr URLs for training images\n",
        "train_url = train['train_url']\n",
        "train_url = np.array(train_url)\n",
        "#dictionary for converting words to vocabulary indices\n",
        "word_code = train['word_code']\n",
        "word_code = np.array(word_code)\n",
        "#Captions for testing images\n",
        "test_caps = test['test_caps']\n",
        "test_caps = np.array(test_caps)\n",
        "#The indices of testing images\n",
        "test_imid = test['test_imid']\n",
        "test_imid = np.array(test_imid) \n",
        "#Pretrained feature vector for testing images !!!USING IT WILL HALVE THE GRADE!!!!\n",
        "test_ims = test['test_ims']\n",
        "test_ims = np.array(test_ims)\n",
        "#Flickr URLs for testing images\n",
        "test_url = test['test_url']\n",
        "test_url = np.array(test_url)\n",
        "\n",
        "#convert words to map indices \n",
        "word_code[0].tolist()\n",
        "word_indices = word_code[0].tolist()\n",
        "word_indices = np.array(word_indices, dtype='int')\n",
        "#convert words to np array\n",
        "words = np.array(word_code.dtype.descr)[:,0]\n",
        "\n",
        "#retrieve all images to variables to use in further processes \n",
        "test_img_path = np.array(listdir(\"/content/content/drive/MyDrive/Dataset/test\"))\n",
        "train_img_path =np.array(listdir(\"/content/content/drive/MyDrive/Dataset/train\"))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ1WhZ0xzzix"
      },
      "outputs": [],
      "source": [
        "#Function to map index to real world\n",
        "def getCaps(id):\n",
        "    captionMatrix = []\n",
        "    for i in np.where(train_imid == id)[0]:\n",
        "        captionMatrix.append(train_cap[i, :])\n",
        "    return np.array(captionMatrix)\n",
        "#Function to map index to real world for test data\n",
        "def getCapsTest(id):\n",
        "    captionMatrix = []\n",
        "    for i in np.where(test_imid == id)[0]:\n",
        "        captionMatrix.append(test_caps[i, :])\n",
        "    return np.array(captionMatrix)\n",
        "\n",
        "#Function to map index to string array\n",
        "def readCaps(captionMatrix):\n",
        "    str = \"\"\n",
        "    str_arr = []\n",
        "    for k in captionMatrix:\n",
        "        str = \"\"\n",
        "        for i in k:\n",
        "          indices = np.where(word_indices == i)[0][0]\n",
        "          word = words[indices]\n",
        "          if i == k[-1]:\n",
        "              str = str + word\n",
        "          else:\n",
        "              str = str + word + \" \"\n",
        "        str_arr.append(str)\n",
        "    return str_arr\n",
        "#Image loader for generating pickle file for train dataset\n",
        "#Pickle file increases the process speed of data in training\n",
        "def imloader(dataset):\n",
        "    batch_size=500\n",
        "    feature_lst = None\n",
        "    index_lst = None\n",
        "    captions = None\n",
        "    with open(\"train_data.pkl\", \"wb\") as outfile:\n",
        "      for data in tqdm(dataset.batch(batch_size)):\n",
        "          image = data[0]\n",
        "          index = data[1].numpy()\n",
        "          feature = inceptionv3_model(image).numpy()\n",
        "\n",
        "          for i in range(feature.shape[0]):\n",
        "              feature_lst = (feature[i].squeeze().reshape((64,2048)))\n",
        "              index_lst = (index[i])\n",
        "              captions = (np.array(getCaps(index[i])))\n",
        "              pickle.dump((feature_lst, index_lst, captions), outfile)\n",
        "      outfile.close()\n",
        "      return None\n",
        "\n",
        "#Image loader for generating pickle file for validation dataset\n",
        "#Pickle file increases the process speed of data in validation      \n",
        "def imloader_val(dataset):\n",
        "    batch_size=500\n",
        "    feature_lst = None\n",
        "    index_lst = None\n",
        "    captions = None\n",
        "    with open(\"test_data.pkl\", \"wb\") as outfile:\n",
        "      for data in tqdm(dataset.batch(batch_size)):\n",
        "\n",
        "          image = data[0]\n",
        "          index = data[1].numpy()\n",
        "          feature = inceptionv3_model(image).numpy()\n",
        "          for i in range(feature.shape[0]):\n",
        "              feature_lst = (feature[i].squeeze().reshape((64,2048)))\n",
        "              index_lst = (index[i])\n",
        "              captions = (np.array(getCaps(index[i])))\n",
        "              pickle.dump((feature_lst, index_lst, captions), outfile)\n",
        "\n",
        "      outfile.close()\n",
        "    return None\n",
        "\n",
        "readCaps(getCapsTest(53))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXr9t1SHPw1q"
      },
      "outputs": [],
      "source": [
        "#Getting pretrained InceptionV3 model \n",
        "inception = InceptionV3(weights='imagenet')\n",
        "inceptionv3_model = Model(inception.input, inception.layers[-3].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az9EBuEUPW1z"
      },
      "outputs": [],
      "source": [
        "#Data loading and train validation split 80%-20%\n",
        "trainDataset = tf.data.Dataset.list_files(\"/content/content/drive/MyDrive/Dataset/train/*.jpg\", shuffle=True).map(lambda x: tf.py_function(load_image_inception, [x], [tf.float32, tf.int32]))\n",
        "trainData = trainDataset.take(int(len(train_img_path)*0.80))\n",
        "validationData = trainDataset.skip(int(len(train_img_path)*0.80))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkGSQnw-ScwJ"
      },
      "outputs": [],
      "source": [
        "if(not flag):\n",
        "  #Batchwise pickle generation\n",
        "  imloader(trainData)\n",
        "  imloader_val(validationData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o0p60Z2XCbr"
      },
      "outputs": [],
      "source": [
        "#Data yield operation definition for Dataset object of Tensorflow\n",
        "def data_loader(filename):\n",
        "   with open(filename, \"rb\") as f:\n",
        "        while 1:\n",
        "            try:\n",
        "\n",
        "                yield pickle.load(f)\n",
        "            except EOFError:\n",
        "                break\n",
        "\n",
        "trainDataset_inception = tf.data.Dataset.from_generator(data_loader, args=['/content/drive/MyDrive/Dataset/train_data.pkl'], output_types=(tf.float32,tf.int32, tf.int32))\n",
        "valDataset_inception = tf.data.Dataset.from_generator(data_loader, args=['/content/drive/MyDrive/Dataset/test_data.pkl'], output_types=(tf.float32,tf.int32, tf.int32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2nRQVYGjr7L"
      },
      "outputs": [],
      "source": [
        "train_set_size = int(len(train_img_path)*0.8) \n",
        "#initilize settings for training\n",
        "sample_set_size, embedding_dim, neuron_count = 3000, 256, 512\n",
        "num_steps  = train_set_size // sample_set_size\n",
        "# Inceptionv3 gives vector in dimention (64,2048)\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n",
        "\n",
        "#implementation of attention mechanism\n",
        "#inspired from https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "#implementation from Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\n",
        "# can be accessed here https://arxiv.org/pdf/1502.03044.pdf\n",
        "class Attention(tf.keras.Model):\n",
        "  def __init__(self, neuron_count):\n",
        "    super(Attention, self).__init__()\n",
        "    #there are 3 dense layers\n",
        "    self.W2 = tf.keras.layers.Dense(neuron_count)\n",
        "    self.W1 = tf.keras.layers.Dense(neuron_count)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "    self.neuron_count = neuron_count\n",
        "  def call(self, features, hidden):\n",
        "\n",
        "    #prepare dimention to initilize it as 1x256 values which will be given to embedding matrix as a coefficient   \n",
        "    attention_pre_step = tf.nn.tanh(self.W1(features) + self.W2(tf.expand_dims(hidden, 1))) \n",
        "    \n",
        "    #print(attention_pre_step.shape)\n",
        "    #generate attention scores\n",
        "    score = self.V(attention_pre_step)\n",
        "    \n",
        "\n",
        "    #calculate soft attention from given scores \n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    #distribution of soft attention for each vector 1x256\n",
        "    vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
        "    #print(vector.shape)\n",
        "    return vector, attention_weights\n",
        "#implementation of encoder mechanism\n",
        "#inspired from https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "class Encoder(tf.keras.Model):\n",
        "    #decreasing output of inceptionv3 model to make it fit into embedding matrix\n",
        "    def __init__(self, embedding_row):\n",
        "        super(Encoder, self).__init__()\n",
        "        #one fully connected layer is enough for understanding relation\n",
        "        self.fc = tf.keras.layers.Dense(embedding_row)\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        #use ReLU activation function\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "#implementation of decoder mechanism\n",
        "#inspired from https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_row, neuron_count, vocab_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.attention = Attention(neuron_count)\n",
        "    #create embedding layer as matrix of shape vocabulary size 1004 x 256\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_row)\n",
        "\n",
        "    self.gru = tf.keras.layers.GRU(neuron_count, return_state=True,\n",
        "                                   return_sequences=True, recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    #sequention 2 fully connected layers to better understand word, feature interaction\n",
        "    self.fc1 = tf.keras.layers.Dense(neuron_count)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "    self.neuron_count = neuron_count\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    #get result of attention weights and vectors from attention block\n",
        "    vector,_ = self.attention(features, hidden)\n",
        "    \n",
        "    y = self.embedding(x)\n",
        "    #print(vector.shape)\n",
        "    #print(y.shape)\n",
        "    #pass sentence input through word embedding and concat embedding output with attention vectors\n",
        "    x = tf.concat([tf.expand_dims(vector,1), self.embedding(x)], axis=-1)\n",
        "\n",
        "    #give input of concated features to GRU layer \n",
        "    out, state = self.gru(x)\n",
        "    \n",
        "    #Pass data through fully connected layers and set shape according to desired output (1x17)\n",
        "    x = self.fc1(out)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "    x = self.fc2(x)\n",
        "    return x, state\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.neuron_count))\n",
        "#Create adam optimizer which performs well on similar tasks\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "#Use Sparse Categorical Crossentropy as loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "encoder = Encoder(embedding_dim)\n",
        "decoder = Decoder(embedding_dim, neuron_count, 1004)\n",
        "\n",
        "#loss function calculation \n",
        "def loss_function(real, pred):\n",
        "  loss_call = loss_object(real, pred)\n",
        "  loss_result = loss_call * tf.cast(tf.math.logical_not(tf.math.equal(real, 0)), dtype=loss_call.dtype)\n",
        "  average_loss = tf.reduce_mean(loss_result)\n",
        "  return average_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI4F9BWqNQJk"
      },
      "outputs": [],
      "source": [
        "#!cp -av  '/content/checkpoints/caglar_full_train2' \"/content/drive/MyDrive/Dataset/checkpoints/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVE9E_dfVitL"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"./checkpoints/attention_with_gru\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7GG0cqnjyaZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "start_epoch = 0\n",
        "#restore the latest checkpoint if there is any\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  path_to_latest_checkpoint = ckpt_manager.latest_checkpoint\n",
        "  last_index_of_checkpoint = path_to_latest_checkpoint.split('-')\n",
        "  start_epoch = int(last_index_of_checkpoint[-1])\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "\n",
        "if(flag):\n",
        "  ckpt.restore(\"/content/drive/MyDrive/Dataset/checkpoints/attention_with_gru/ckpt-4\")\n",
        "  start_epoch = 7\n",
        "\n",
        "loss_plot = []\n",
        "\n",
        "@tf.function\n",
        "def train_step(img_tensor, ground_truth):\n",
        "  loss = 0\n",
        "  caption_count = ground_truth.shape[0]\n",
        "  caption_length = ground_truth.shape[1]\n",
        "  temp_caption = decoder.reset_state(batch_size=caption_count)\n",
        "\n",
        "  dec_input = tf.expand_dims([1] * caption_count, 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      #pass inceptionv3 outputs through encoder\n",
        "      features = encoder(img_tensor)\n",
        "      for i in range(1, caption_length):\n",
        "          #get output of decoder \n",
        "          predictions, temp_caption = decoder(dec_input, features, temp_caption)\n",
        "          loss += loss_function(ground_truth[:, i], predictions)\n",
        "          #populate words to add into decoder input as next step\n",
        "          dec_input = tf.expand_dims(ground_truth[:, i], 1)\n",
        "\n",
        "  #declare conponents for backpropagation to calculate gradients\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "  return loss, (loss / int(caption_length))\n",
        "\n",
        "num_of_total_epochs = 7\n",
        "\n",
        "for epoch in range(start_epoch, num_of_total_epochs):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    avg_batch_loss_total = 0\n",
        "    counter = 0\n",
        "    #epoch += 1\n",
        "    for (batch, (img_tensor, index, ground_truth)) in enumerate(trainDataset_inception):\n",
        "        batch_loss, t_loss = train_step(img_tensor, ground_truth)\n",
        "        total_loss = total_loss + t_loss\n",
        "        counter += 1\n",
        "        if batch % sample_set_size == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(ground_truth.shape[1])\n",
        "            avg_batch_loss_total += average_batch_loss\n",
        "            print('Epoch {} Batch {} Loss {}'.format(epoch+1,batch,average_batch_loss))\n",
        "\n",
        "      \n",
        "    loss_plot.append(avg_batch_loss_total / num_steps) \n",
        "    #save each epoch \n",
        "    ckpt_manager.save()\n",
        "\n",
        "    print('Epoch {} Loss {}'.format(epoch+1, total_loss/num_steps ))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time()-start))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HIGr-YgJLUGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30W46iTyZTfV"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def validation(img_tensor, target):\n",
        "    loss = 0\n",
        "    caption_count = ground_truth.shape[0]\n",
        "    caption_length = ground_truth.shape[1]\n",
        "    temp_caption = decoder.reset_state(batch_size=caption_count)\n",
        "    dec_input = tf.expand_dims([1] * caption_count, 1)\n",
        "    with tf.GradientTape() as tape:\n",
        "        #pass inceptionv3 outputs through encoder\n",
        "        features = encoder(img_tensor)\n",
        "        for i in range(1, caption_length):\n",
        "            #get output of decoder \n",
        "            predictions, temp_caption = decoder(dec_input, features, temp_caption)\n",
        "            loss += loss_function(ground_truth[:, i], predictions)\n",
        "            #populate words to add into decoder input as next step\n",
        "            dec_input = tf.expand_dims(ground_truth[:, i], 1)\n",
        "    return loss, (loss / int(caption_length))"
      ],
      "metadata": {
        "id": "dBypRNitzZju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(not flag):\n",
        "  num_of_total_epochs = 7\n",
        "  validation_loss = []\n",
        "  for epoch in range(1, num_of_total_epochs):\n",
        "      \n",
        "      path = ckpt_manager.latest_checkpoint\n",
        "      index = path.find(\"ckpt-\")\n",
        "      path = path[:index+5] + str(epoch) + path[index + 6:]\n",
        "      ckpt.restore(path)\n",
        "      start = time.time()\n",
        "      total_loss = 0\n",
        "      avg_batch_loss_total = 0\n",
        "      counter = 0\n",
        "      #epoch += 1\n",
        "      for (batch, (img_tensor, index, targets)) in enumerate(valDataset_inception):\n",
        "          batch_loss, t_loss = validation(img_tensor, targets)\n",
        "          total_loss += t_loss\n",
        "          counter += 1\n",
        "          if batch % sample_set_size == 0:\n",
        "              average_batch_loss = batch_loss.numpy()/int(targets.shape[1])\n",
        "              avg_batch_loss_total += average_batch_loss\n",
        "              print('Epoch {} Batch {} Loss {}'.format(epoch,batch,average_batch_loss))\n",
        "      print(total_loss / counter)\n",
        "      validation_loss.append(total_loss / counter)\n",
        "      "
      ],
      "metadata": {
        "id": "He6e-Orpz-JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(not flag):\n",
        "  temp_loss = []\n",
        "  for y, loss in enumerate(validation_loss):\n",
        "    temp_loss.append(loss)\n",
        "  plt.plot(loss_plot, label = 'Train Loss')\n",
        "  plt.plot(temp_loss, label = 'Validation Loss')\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Sparse Categorical Crossentropy\")\n",
        "  plt.title(\"Sparse Categorical Crossentropy vs Epoch for Validation and Train Data\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "E_8aZ2aJ-jWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mXeUAzIwauu"
      },
      "outputs": [],
      "source": [
        "valgen = data_loader(\"/content/drive/MyDrive/Dataset/test_data.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(100):\n",
        "#   iterated = next(valgen)"
      ],
      "metadata": {
        "id": "kvGXq_x3qcKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jzqw6zx7TQki",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "9f1a8118-697d-4c85-8c0d-25f2c15fdaa1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-859a720641a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0miterated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/content/drive/MyDrive/Dataset/train/\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mcapt_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadCaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetCaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-859a720641a8>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#make prediction for all words iteratively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpred_categoric_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpred_categoric_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_categoric_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-b7091f88d43e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, features, hidden)\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m#get result of attention weights and vectors from attention block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-b7091f88d43e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, features, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#prepare dimention to initilize it as 1x256 values which will be given to embedding matrix as a coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mattention_pre_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#print(attention_pre_step.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"attention_2\" (type Attention).\n\nIn this `tf.Variable` creation, the initial value's shape ((256, 256)) is not compatible with the explicitly supplied `shape` argument ((256, 512)).\n\nCall arguments received:\n  • features=tf.Tensor(shape=(1, 64, 256), dtype=float32)\n  • hidden=tf.Tensor(shape=(1, 512), dtype=float32)"
          ]
        }
      ],
      "source": [
        "def evaluate(image):\n",
        "    #prepare input\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "    inception_out = inceptionv3_model(tf.expand_dims(load_image_inceptionEval(image)[0],0))\n",
        "    inception_out = tf.reshape(inception_out, (inception_out.shape[0], -1, inception_out.shape[3]))\n",
        "    #pass through encoder\n",
        "    features = encoder(inception_out)\n",
        "    temp_input = tf.expand_dims(tf.expand_dims(1, 0), axis=0)\n",
        "    result = []\n",
        "    #make prediction for all words iteratively\n",
        "    for i in range(17):\n",
        "        predictions, hidden = decoder(temp_input,features,hidden)\n",
        "        pred_categoric_out = tf.random.categorical(predictions, 1)\n",
        "        pred_categoric_out = pred_categoric_out[0][0]\n",
        "        ids = pred_categoric_out.numpy()\n",
        "        dec_output_word = tf.compat.as_text(words[np.where(word_indices == ids)][0])\n",
        "        result.append(dec_output_word)\n",
        "        if dec_output_word == 'x_END_':\n",
        "            return result\n",
        "        temp_input = tf.expand_dims([ids], 0)\n",
        "    return result\n",
        "#call eval function with given data\n",
        "iterated = next(valgen)\n",
        "image = \"/content/content/drive/MyDrive/Dataset/train/\"+ str(iterated[1])+\".jpg\"\n",
        "result = evaluate(image)\n",
        "res = imread(image)\n",
        "capt_real = readCaps(getCaps(iterated[1]))\n",
        "temp_cap = capt_real[0]\n",
        "temp_cap = temp_cap[9:temp_cap.find(' x_END_')]\n",
        "plt.title('Real Caption:'+temp_cap + '\\n'+'Prediction Caption:' + ' '.join(result[:-1]))\n",
        "plt.imshow(res)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3WuMu9E1RLb"
      },
      "outputs": [],
      "source": [
        "def evaluate_test(image):\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "    #pass through encoder\n",
        "    features = encoder(image)\n",
        "    temp_input = tf.expand_dims(tf.expand_dims(1, 0), axis=0)\n",
        "    result = []\n",
        "    #make prediction for all words iteratively\n",
        "    for i in range(17):\n",
        "        predictions, hidden = decoder(temp_input,features,hidden)\n",
        "        pred_categoric_out = tf.random.categorical(predictions, 1)\n",
        "        pred_categoric_out = pred_categoric_out[0][0]\n",
        "        ids = pred_categoric_out.numpy()\n",
        "        dec_output_word = tf.compat.as_text(words[np.where(word_indices == ids)][0])\n",
        "        result.append(dec_output_word)\n",
        "        if dec_output_word == 'x_END_':\n",
        "            return result\n",
        "        temp_input = tf.expand_dims([ids], 0)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYVKpGE32eul"
      },
      "outputs": [],
      "source": [
        "!pip install pycocoevalcap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_sMWNPhBTbG"
      },
      "outputs": [],
      "source": [
        "def test_data_loader(dataset, batch_size):\n",
        "    feature_lst = []\n",
        "    index_lst = []\n",
        "    captions = []\n",
        "    for data in tqdm(dataset.batch(batch_size).take(100)):\n",
        "\n",
        "        image = data[0]\n",
        "        index = data[1].numpy()\n",
        "        feature = inceptionv3_model(image).numpy()\n",
        "\n",
        "        for i in range(feature.shape[0]):\n",
        "            feature_lst.append(feature[i].squeeze().reshape((64,2048)))\n",
        "            index_lst.append(index[i])\n",
        "            captions.append(np.array(getCapsTest(index[i])))\n",
        "\n",
        "    return feature_lst, index_lst, captions\n",
        "\n",
        "test_img_path = np.array(listdir(\"/content/content/drive/MyDrive/Dataset/test\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testDataset = tf.data.Dataset.list_files(\"/content/content/drive/MyDrive/Dataset/test/*.jpg\", shuffle=True).map(lambda x: tf.py_function(load_image_inception, [x], [tf.float32, tf.int32]))\n",
        "testDataset_subset = testDataset.take(int(len(test_img_path)*0.01))"
      ],
      "metadata": {
        "id": "EBwYk1MYLpMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_test, index_test, caption_test = test_data_loader(testDataset_subset,100)"
      ],
      "metadata": {
        "id": "J8ISBqiDMM7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1ZmpVfF03_C"
      },
      "outputs": [],
      "source": [
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider \n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "\n",
        "def score(ground_truth, prediction):\n",
        "    \n",
        "\n",
        "    score_dict = {}\n",
        "    metrics = [(Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
        "                (Meteor(),\"METEOR\"),\n",
        "                (Rouge(), \"ROUGE_L\"),\n",
        "                (Cider(), \"CIDEr\")]\n",
        "    for scorer, method in metrics:\n",
        "\n",
        "        score, scores = scorer.compute_score(ground_truth, prediction)\n",
        "\n",
        "        if type(score) == list:\n",
        "            for m, s in zip(method, score):\n",
        "\n",
        "                score_dict[m] = s\n",
        "\n",
        "        else:  \n",
        "            score_dict[method] = score\n",
        "\n",
        "    b1,b2,b3,b4 = score_dict[\"Bleu_1\"] * 100, score_dict[\"Bleu_2\"] * 100, score_dict[\"Bleu_3\"] * 100, score_dict[\"Bleu_4\"] * 100\n",
        "    m, r, c = score_dict[\"METEOR\"] * 100, score_dict[\"ROUGE_L\"] * 100,  score_dict[\"CIDEr\"] * 100\n",
        "    blue_scores = \"BLEU-1: {} BLEU-2: {} BLEU-3: {} BLEU-4 {} \".format(b1,b2,b3,b4)\n",
        "    other_scores = \"METEOR:  {} ROGUE_L: {} CIDEr: {} \".format(m, r, c)\n",
        "    print(blue_scores)\n",
        "    print(other_scores)\n",
        "    return score_dict\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GG1yxzoql2u"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYVXxijthfu_"
      },
      "outputs": [],
      "source": [
        "start = 0\n",
        "indexes = index_test\n",
        "blue_1 = 0\n",
        "blue_2 = 0\n",
        "blue_3 = 0\n",
        "blue_4 = 0\n",
        "meteor = 0\n",
        "rouge_l = 0\n",
        "cider = 0\n",
        "counter = 0\n",
        "for i, index in enumerate(indexes):\n",
        "  predictions = {}\n",
        "  gt_caption = {}\n",
        "  captions = caption_test[i+start]\n",
        "  testim = imread('/content/content/drive/MyDrive/Dataset/test/'+str(index)+'.jpg')\n",
        " \n",
        "  for k in range(len(captions)):\n",
        "    prediction_array_of_words = evaluate_test(features_test[i+start])\n",
        "    pred_sentence = [' '.join(prediction_array_of_words[:-1])]\n",
        "    predictions['0'] = pred_sentence\n",
        "    capt_real = readCaps(getCapsTest(index))[k]\n",
        "    capt_real = capt_real[9:capt_real.find(' x_END_')]\n",
        "    gt_caption['0'] = [capt_real]\n",
        "    print(\"################SEPERATOR###########\")\n",
        "    print(gt_caption)\n",
        "    print(predictions)\n",
        "    plt.title('Real Caption:'+ capt_real + '\\n'+'Prediction Caption:' + pred_sentence[0])\n",
        "    plt.imshow(testim)\n",
        "    plt.show()  \n",
        "    #print(temp)\n",
        "    score_output = score(predictions, gt_caption)\n",
        "    blue_1 +=  score_output['Bleu_1']\n",
        "    blue_2 +=  score_output['Bleu_2']\n",
        "    blue_3 +=  score_output['Bleu_3']\n",
        "    blue_4 +=  score_output['Bleu_4']\n",
        "    meteor +=  score_output['METEOR']\n",
        "    rouge_l +=  score_output['ROUGE_L']\n",
        "    cider +=  score_output['CIDEr']\n",
        "    counter += 1\n",
        "\n",
        "    #print(gt_caption[counter] )\n",
        "blue_1 = blue_1 /  counter \n",
        "blue_2 = blue_2 /  counter \n",
        "blue_3 = blue_3 /  counter \n",
        "blue_4 = blue_4 /  counter \n",
        "meteor = meteor /  counter \n",
        "rouge_l = rouge_l / counter\n",
        "cider = cider / counter\n",
        "print(f\"Blue 1 {blue_1}, Blue 2 {blue_2} ,Blue 3 {blue_3},Blue 4 {blue_4}, Meteor {meteor}, rouge_l {rouge_l}, cider {cider}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7SoKzD-2hae"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "UPLOAD_ATTENTION_GRU_GROUP6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}